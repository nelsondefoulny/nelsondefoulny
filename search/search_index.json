{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>I am a developer passionate about Unreal Engine and real-time technologies. I enjoy exploring a wide variety of topics personally and professionally and I wanted to share my projects here.</p> <p>You can also explore my project by tag here</p>"},{"location":"#highlights","title":"Highlights","text":"Virtual Production"},{"location":"#professional-projects","title":"Professional Projects","text":"MSG Sphere Projection Pavillon Virtual Production Pirata Et Capitano Pipeline Motion Capture UE Animation Pipeline Expert Road Safety Smart Mirror Kalulu"},{"location":"#personal-projects","title":"Personal Projects","text":""},{"location":"#tutorials","title":"Tutorials","text":"UE HLSL in Materials UE Ray Marching Introduction UE Ray Marching Lava Lamp"},{"location":"about/","title":"About","text":"<p>Welcome, I am a developer passionate about Unreal Engine and real-time technologies in general.</p> <p>Over the years, I have held various roles centered around game and interactive experience development using Godot, Unity, and Unreal Engine, with a growing specialization in Unreal Engine. </p> <p>I then transitioned to a Technical Director role in Animation Production, where I integrated motion capture and real-time rendering into the animation pipeline. </p> <p>Later, I joined a Virtual Production company as Technical Director, tackling complex challenges while managing and developing tools for content creation and virtual production setup and operation. </p> <p>I also work as a freelancer on various projects and teach high school classes.</p> <p></p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:blueprint","title":"Blueprint","text":"<ul> <li>            Expert Road Safety          </li> <li>            MSG Sphere Projection          </li> <li>            Motion Capture          </li> <li>            Pavillon          </li> <li>            Smart Mirror          </li> <li>            UE Animation Pipeline          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:c","title":"C#","text":"<ul> <li>            Smart Mirror          </li> </ul>"},{"location":"tags/#tag:c","title":"C++","text":"<ul> <li>            Expert Road Safety          </li> <li>            MSG Sphere Projection          </li> <li>            Motion Capture          </li> <li>            Pavillon          </li> <li>            Smart Mirror          </li> <li>            UE Animation Pipeline          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:gdscript","title":"GDScript","text":"<ul> <li>            Kalulu          </li> </ul>"},{"location":"tags/#tag:git","title":"Git","text":"<ul> <li>            Expert Road Safety          </li> <li>            Kalulu          </li> <li>            Motion Capture          </li> <li>            Pirata Et Capitano Pipeline          </li> <li>            Smart Mirror          </li> <li>            UE Animation Pipeline          </li> </ul>"},{"location":"tags/#tag:godot","title":"Godot","text":"<ul> <li>            Kalulu          </li> </ul>"},{"location":"tags/#tag:hlsl","title":"HLSL","text":"<ul> <li>            UE HLSL in Materials          </li> <li>            UE Ray Marching Introduction          </li> <li>            UE Ray Marching Lava Lamp          </li> </ul>"},{"location":"tags/#tag:network","title":"Network","text":"<ul> <li>            Pavillon          </li> </ul>"},{"location":"tags/#tag:opencv","title":"OpenCV","text":"<ul> <li>            Smart Mirror          </li> </ul>"},{"location":"tags/#tag:plastic-scm","title":"Plastic SCM","text":"<ul> <li>            MSG Sphere Projection          </li> <li>            Pavillon          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            Kalulu          </li> <li>            Motion Capture          </li> <li>            Pirata Et Capitano Pipeline          </li> <li>            UE Animation Pipeline          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:shotgun","title":"Shotgun","text":"<ul> <li>            Pirata Et Capitano Pipeline          </li> <li>            UE Animation Pipeline          </li> </ul>"},{"location":"tags/#tag:unity","title":"Unity","text":"<ul> <li>            Smart Mirror          </li> </ul>"},{"location":"tags/#tag:unreal-engine-4","title":"Unreal Engine 4","text":"<ul> <li>            Expert Road Safety          </li> <li>            Motion Capture          </li> <li>            Smart Mirror          </li> <li>            UE Animation Pipeline          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:unreal-engine-5","title":"Unreal Engine 5","text":"<ul> <li>            MSG Sphere Projection          </li> <li>            Pavillon          </li> <li>            UE HLSL in Materials          </li> <li>            UE Ray Marching Introduction          </li> <li>            UE Ray Marching Lava Lamp          </li> <li>            Virtual Production          </li> </ul>"},{"location":"tags/#tag:vr","title":"VR","text":"<ul> <li>            Expert Road Safety          </li> </ul>"},{"location":"posts/professional_projects/bn_msg_sphere_projection/","title":"MSG Sphere Projection","text":"","tags":["Unreal Engine 5","Blueprint","C++","Plastic SCM"]},{"location":"posts/professional_projects/bn_msg_sphere_projection/#description","title":"Description","text":"<p>I developed for Blue Node and one of its clients an Unreal Engine plugin designed to assist with MSG Sphere outer screen media production. The main motivation was to streamline the creation of anamorphic projections onto a 360 render. In non-technical terms, the idea was to give the impression that the 3D object was inside the sphere. Additionally, it was critical to provide a real-time preview of the final render because the shape of the display makes it difficult to work with a traditional linear media layer workflow.</p> <p></p> <p></p>","tags":["Unreal Engine 5","Blueprint","C++","Plastic SCM"]},{"location":"posts/professional_projects/bn_msg_sphere_projection/#technical-information","title":"Technical Information","text":"<p>360 rendering is a topic all on its own. The native cube capture and all the 360 camera plugins I tried had some restrictions, especially regarding resolution and post-process effects. Due to time constraints, I couldn't develop my own 360 camera, so I simply developed an interface with some overhead so the plugin can use different 360 cameras natively. I chose this approach because plugin constraints were often mutually exclusive, meaning there is always a good camera for a specific set of rendering functionalities required.</p> <p>For anamorphic rendering, I used a similar approach, using cine cameras for quality and scene captures for previews.</p> <p>The projection and media blending is just a simple shader with a bit of math. This could have been more tricky to do for a random mesh, but as the display is a sphere, it simplifies the projection problem a lot.</p> <p>Getting the core of the plugin working was not too difficult, even if I spent some time ensuring which post-process effects were supported. I also encountered a few issues guaranteeing that all rendered images are in linear color space.</p> <p>The main difficulty was actually implementing a graphic designer workflow that is both intuitive and flexible. The final tool can layer and blend images from any number of 360 cameras and any number of anamorphic projectors. In practice, each camera has multiple configurations: what the camera sees (filtering scene objects), how to render (camera settings, post-process, etc.), and how to blend on the sphere (compositing workflow using materials). I also spent a lot of time ensuring the final image export and the real-time preview matched visually while maintaining their own set of options.</p>","tags":["Unreal Engine 5","Blueprint","C++","Plastic SCM"]},{"location":"posts/professional_projects/bn_pavillon/","title":"Pavillon","text":"","tags":["Unreal Engine 5","Blueprint","C++","Network","Plastic SCM"]},{"location":"posts/professional_projects/bn_pavillon/#description","title":"Description","text":"<p>Pavillon is a Pixel Stream interactive experience that I built with my team for Blue Node. The experience puts the user in a 3D modern design-style environment and allows them to explore and expand the world by adding new elements. The world was persistent and shared, allowing players to collaborate on world extensions or simply see what others had built. Thanks to Pixel Stream technology, the game ran on AWS servers and was streamed to users' smartphones in real time, providing high-quality rendering. Due to server costs, the game was not deployed at a large scale, but it was used at private events for a few dozen players.</p> <p>The most challenging task by far was creating the cloud scaling architecture. It required instantiating and destroying several web servers and Unreal Engine instances as the number of connected players changed. Building such a system required strong teamwork and technical skills from cloud engineers, DevOps, web developers, and game developers.</p> <p>Personally, I mostly worked on the Unreal Engine game application, which can be broken down into three main topics.</p> <p>The first is, of course, the core game mechanics. Placing objects in the world in a user-friendly way is no easy task, especially for a short experience on mobile with users who are not experienced gamers. To achieve this, we built the game using two core concepts. For props, we used an anchor system allowing free placement. For structural elements, we built a multi-size dynamic grid that can accept elements of different sizes while ensuring intuitive placement.</p> <p>The second topic is input handling. We wanted responsive controls, so we decided to get input from the web frontend. We built a generic system to transfer user inputs from the frontend to Unreal Engine by passing through the Pixel Stream server. This was more complicated than it seems due to security restrictions as well as connection limitations between web servers and Unreal Engine.</p> <p>Finally, we worked on the network to ensure the open world was persistent and accessible by multiple users simultaneously. As the game was not designed for large scale nor direct player-to-player interaction, we were able to use Unreal Engine's native network stack without too much overhead.</p>","tags":["Unreal Engine 5","Blueprint","C++","Network","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/","title":"Virtual Production","text":"","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#introduction","title":"Introduction","text":"<p>As the technical director of Blue Node, I led a small developer team. Over the years, we implemented many tools to advance our Virtual Production setups.</p> <p>Virtual Production has many different meanings depending on the industry and context. The most general definition is any production that leverages real-time technology to replace classic offline rendering, providing new options. At Blue Node, each production had its own workflow and tools depending on the needs. Therefore, I had the opportunity to work with a wide range of technologies and devices.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#productions","title":"Productions","text":"<p>Here is a short list of some production I worked on.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#tv","title":"TV","text":"","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#hondelatte-raconte-series-1-2-canal","title":"Hondelatte Raconte Series 1 &amp; 2 Canal+","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#la-soiree-extraordinaire-m6","title":"La soir\u00e9e extraordinaire M6","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#advertising","title":"Advertising","text":"","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#louis-vuitton-eyewear-ss22","title":"Louis Vuitton EyeWear SS22","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#coperni-ss21-campaign","title":"Coperni SS21 Campaign","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#events","title":"Events","text":"","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#fifa-club-world-cup-closing-ceremony-ar-content","title":"FIFA club world cup closing ceremony AR content","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#ubisoft-xr-live-conference","title":"Ubisoft XR Live Conference","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#music","title":"Music","text":"","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#soprano-xr-livestream","title":"Soprano XR Livestream","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#owlle-xr-music-video","title":"Owlle XR Music Video","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#eddy-de-pretto-desole-caroline","title":"Eddy De Pretto D\u00e9sol\u00e9 Caroline","text":"Your browser does not support the video tag.","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#virtual-production-core-concepts","title":"Virtual Production Core Concepts","text":"<p>Explaining virtual production in depth would require a dedicated book, so I will just provide an overview.</p> <p>The main idea behind these productions is to get the physical camera location in real time (Tracking). Using this information, we can compute the 3D camera frustum and render a virtual scene. This gives us an image with the correct perspective from the camera's viewpoint. Depending on the context, we can use the created image to enhance the physical camera's captured image.</p> <p>In the case of Augmented Reality (AR), the rendered image is simply blended in front of the captured image. This is used to add virtual objects in front of actors.</p> <p>In the case of In-Camera VFX (ICVFX), the goal is to send this image to a physical LED wall behind the actors. This creates the same effect as a traditional green screen, but the result is visible immediately instead of waiting for post-production. The second advantage is that the lighting matches the 3D environment, which is especially useful for creating ambiance or handling reflective objects. On top of camera tracking, this technique also requires a 3D mesh that matches the physical LED screen in order to re-project the rendered image.</p> <p>One of the problems with ICVFX is that large shots are not really feasible because the camera will capture the surrounding studio, which we don't want to see. The ideal solution is to use a bigger LED wall, but this is very expensive. The second solution is to create an \"extend\" (XR). The idea is to render the same scene and perspective as the LED wall image and use this as an AR mask to cover part of the image that is outside of the LED screen.</p> <p>All of the productions above leverage camera tracking and Unreal Engine real-time rendering. But the actual setup might include one or multiple concepts (ICVFX / AR / XR).</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#what-it-looks-like-in-practice","title":"What it looks like in practice","text":"<p>A virtual production setup usually includes the following elements: - One or multiple cameras. - One tracking system per tracked camera. - A genlock to synchronize devices. It is a pulse indicating the beginning or end of each frame. - A timecode to get a consistent time source across multiple devices. It can also be used to control some playback or trigger events. - ICVFX: a LED screen and its processors. - ICVFX: a cluster of synchronized rendering servers (LEDs have too many pixels for one machine). The underlying technology is called nDisplay and is developed by Epic Games. - AR: one rendering server per video feed (usually one per camera). - An operator computer that serves as a hub to control servers. - A graphic designer's dedicated computer. - A bunch of networks using a variety of protocols to transfer data across devices (UDP, TCP, DMX, OSC, etc.).</p> <p>This list is just an overview of important devices for the 3D-related tasks. There are many other devices to control other aspects of the production (lights, recording, audio, set, etc.).</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#personal-contribution","title":"Personal Contribution","text":"<p>I want to present a short list of the tools my team and I developed. A big part of the job was linked to the specifications of each production, and we produced a lot of one-off code that is not worth mentioning in detail.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#checks","title":"Checks","text":"<p>One of the principal difficulties in Virtual Production is that it is a long chain of devices and software, and the only feedback is the final image. Identifying the source of a problem or even noticing an existing problem is not trivial. With my team, we developed multiple tools (bash, python, unreal, etc) to help us with diagnosis.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#source-control","title":"Source Control","text":"<p>In order to sync rendering servers, we extended Epic Games' Switchboard to support Plastic SCM. We also added real-time diagnosis of machine performance and potential divergence with other servers.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#operator-control","title":"Operator Control","text":"<p>This is the main Unreal Engine tool we developed. It is more a suite of tools designed to set up and operate the system. Overall features include: - Create/Load per machine configuration. - Load/Save/Sync Unreal Engine maps. - Control and diagnose machine performance in real time. - Centralize messages and error logs from all machines. - Trigger events and sequences. - Complex compositing setup for ICVFX inner and outer frustum. - Handle network communication with third-party devices using OSC, UDP, and DMX protocols.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#led-wall-creator","title":"LED Wall Creator","text":"<p>This tool was made to create LED meshes for ICVFX setups. This was motivated by the difficulty we faced in creating these meshes. In order to have valid projection, we need a very accurate mesh, and all other techniques we tried (measurement, lasers, lidar, etc) failed to provide a quick, accurate, and repeatable creation process.</p> <p>In the first version of the tool, the mesh was built vertically on top of the spline. Spline points were projected in a pixel-accurate way onto the LED floor. Therefore, moving spline points was reflected in real time on the physical floor, which allowed us to create the curved LED wall.</p> <p>For this tool I developed: - 3D to 2D location conversion - Dynamic mesh creation, UV editing, and static mesh baking from Unreal Engine Editor - Automatic reload of nDisplay mesh at runtime (in servers) - Edit propagation through Unreal Engine Multi-User tool</p> <p>I also worked on a second version that did not require an LED floor. The idea was to match 2D and 3D grids in the LED wall with the same grid in AR. Overlapping both grids in the final display allowed us to edit the mesh iteratively until both grids matched. The tool development was eventually canceled, but I had made significant progress, so it is worth mentioning.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#technical-mire","title":"Technical Mire","text":"<p>I developed a technical mire for LED screens. This tool had multiple motivations, but the main idea was to validate LED screen setup. Among other things, it helped a lot to validate LED screen layout and color as well as nDisplay LED mapping. It often happened that some tiles of the LED were inverted either in the LED processor mapping or physically. The same goes for colorimetry, which might be off for multiple reasons. Especially, it makes it visually obvious if the LED is displaying 8-bit rather than 10-bit colors. In terms of development, it is just a shader, but getting pixel-accurate precision wasn't that simple due to the complexity of the post process chain in Unreal Engine.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#scene-snapshot","title":"Scene Snapshot","text":"<p>This tool was built on top of multiple Unreal plugins and was developed to save and organize scene snapshots and replay them on demand. Handling multiple scene versions in Unreal Engine can be quite challenging, especially because of the fast-paced production we were working on. Duplicating a map and using source control was definitely not an option as it would have taken too much time and forced the rendering cluster to reboot. Therefore, we built the tool on top of Epic Games' Level Snapshot tool. We developed a complex filtering setup tied to a sublevel system that holds graphical content, leaving the persistent level free to contain technical elements. We also developed multiple snapshot reloading methods (code, sequence, custom UI). The most complex part was making sure that snapshots reloads properly on the nDisplay Cluster, as the Level Snapshot is an Editor tool running on the operator machine while the servers are in runtime mode.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#multi-unreal-per-server","title":"Multi Unreal Per Server","text":"<p>For Hondelatte Raconte season 2, we wanted to swap scenes in real time to create a theater effect. Normally in ICVFX production, we use the first GPU to render the camera frustum and the second GPU to render a 360 image of the scene for the outer frustum, which serves as ambient light. This is done using one Unreal Engine instance per machine, leveraging nDisplay multi-GPU support.</p> <p>For this production, we decided to use pre-rendered images for the outer frustum, which freed a GPU. I extended the Unreal Engine TextureShare plugin to support texture transfer between two Unreal instances running on the same machine. This allowed me to set up two Unreal instances per server, one on each GPU, each rendering its own scene. I then leveraged texture sharing across GPUs to composite the final image on the first GPU (the one with the display port output).</p> <p>Modifying TextureShare was not trivial, but the real complexity did not come from this part of the plan. The main problem was adding control for loading not one but two scenes per machine and dispatching them properly to both instances. None of Unreal Engine's plugins dedicated to virtual production support such a setup. Therefore, we had to extend the operator control tool, compositing setup, outer frustum images, multi-user behavior, error management, and basically all of the existing tools we were using.</p> <p>This was a big challenge, but during production it worked well and even exceeded expectations by reducing the amount of time lost between scenes thanks to being able to preload scenes in advance. </p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/bn_virtual_production/#graphics-designers-support","title":"Graphics Designers Support","text":"<p>Another important part of my job was supporting graphics designers. I developed multiple small tools related to content creation, such as spline placement tools, conveyor systems, some rendering-related scripts, and of course some turnkey solutions to integrate their work in the context of virtual production.</p>","tags":["Unreal Engine 4","Unreal Engine 5","Blueprint","C++","Python","Plastic SCM"]},{"location":"posts/professional_projects/mm_motion_capture/","title":"Motion Capture","text":"","tags":["Unreal Engine 4","Blueprint","C++","Python","Git"]},{"location":"posts/professional_projects/mm_motion_capture/#description","title":"Description","text":"<p>I developed for Millimages a real-time performance capture preview in Unreal Engine 4 using Ikinema, XSens, ManusVR, and Dynamixyz technologies.</p> <p>I developed a set of tools in the Unreal Engine 4 Editor to solve pipeline issues such as importing Maya layouts and Redshift materials with their associated textures. I also worked on rendering, props, collisions, interactions, and 2D facial animations. The main goal was to import resources into Unreal Engine to enable live previews of motion capture in real time, even though the production pipeline was based on Maya.</p> <p>On the animation side, we developed several tools to clean up motion capture sampling, isolate key animation points, and bake animation curves onto an animator-friendly rig in Maya. Despite all these tools, there was still a lot of manual work required from animators. For simple animations, motion capture and traditional animation had almost the same cost, but this solution was significantly cheaper for complex animations like dancing and fighting.</p> <p></p> <p> </p>","tags":["Unreal Engine 4","Blueprint","C++","Python","Git"]},{"location":"posts/professional_projects/mm_pirata_et_capitano/","title":"Pirata Et Capitano Pipeline","text":"","tags":["Python","Shotgun","Git"]},{"location":"posts/professional_projects/mm_pirata_et_capitano/#description","title":"Description","text":"<p>Pirata et Capitano is a French and Italian animated series.</p> <p>I worked on season 2, which was produced by Millimages and consists of 52 episodes of 11 minutes each.</p> <p>We used Shotgun as a database, and the pipeline also relied on Shotgun Toolkit. I led a small team of developers responsible for creating interfaces between artists' tools and the database to ensure data integrity and correctness. We also developed and integrated Shotgun Toolkit core applications to provide a simplified environment for tools developers.</p>","tags":["Python","Shotgun","Git"]},{"location":"posts/professional_projects/mm_ue_animation_pipeline/","title":"UE Animation Pipeline","text":"","tags":["Unreal Engine 4","Shotgun","Blueprint","C++","Python","Git"]},{"location":"posts/professional_projects/mm_ue_animation_pipeline/#description","title":"Description","text":"<p>I developed for Millimages a production pipeline for animation that leverages both classic DCC software like Maya and Unreal Engine 4 technology for real-time rendering. Production tracking was linked to the Shotgun database. The pipeline also used motion capture to speed up animation creation.</p> <p>This pipeline has been used to create several in-house 3D animation trailers.</p> <p>Technically, the pipeline used Maya for modeling, rigging, and animation, while shading, layout, lighting, and rendering were done using Unreal Engine 4. Achieving offline rendering quality with Unreal Engine 4 is a tough challenge but also comes with several advantages. As we work on the final image from the start, directors and artists can spot problems early and fix them before too much dependent work has been done. Realtime workflow is based on iterative integration of assets and shots, which partially removes the cost of maintaining multiple versions of the same resource. These two main advantages significantly lower the cost of retakes. Switching from a linear production to an iterative workflow leads to better quality, storytelling, and production cost.</p> <p></p> <p> </p>","tags":["Unreal Engine 4","Shotgun","Blueprint","C++","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/","title":"Kalulu","text":"","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#description","title":"Description","text":"<p>Kalulu Education is a serious game designed for learning reading and math skills, using the latest research in cognitive science. It was developed by Manzalab and Inserm for the French national education system.</p> <p>This Android game was developed using the Godot Engine. The game is composed of a learning module and many different mini-games. The process is to present concepts to children and encourage them to practice through games. Everything is designed to be very playful.</p> <p>The game is built around graphemes and phonemes. The learning module presents grapheme-phoneme associations to children, and the mini-games are built to help them practice and deeply understand these concepts. One of the challenges is that game data (grapheme-phoneme, words, sentences, sounds, etc.) comes from Inserm and is fed into the game during compilation. This allows for testing multiple setups and comparing children's progression results.</p> <p></p> <p> </p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#menu-and-progression","title":"Menu and Progression","text":"<p>Main Menu</p> <p> </p> <p>Progression Overview</p> <p></p> <p>Progression Menu</p> <p></p> <p>Lesson Menu</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#learning-module","title":"Learning Module","text":"","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#practice-games","title":"Practice Games","text":"<p>Crabs</p> <p></p> <p>Jellyfish</p> <p> </p> <p>Caterpillar</p> <p></p> <p>Parrots</p> <p> </p> <p>Monkeys</p> <p></p> <p>Frogs</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#personal-contribution","title":"Personal Contribution","text":"<p>My main tasks on this project were: - Developing the learning module - Developing the progression menu - Validating the learning data consistency</p> <p>I also provided support to other developers when there were issues with Godot (beta version) that required C++ knowledge.</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#learning-module_1","title":"Learning Module","text":"<p>The learning module presents a grapheme-phoneme association, demonstrates how to write it, and asks the player to trace the grapheme in four different forms (lowercase/uppercase, print/cursive).</p> <p>The first two features aren't too difficult, but the tracing mini-game is more challenging. The learning purpose implies that the tracing has a specific direction, start, and end position. Since the game is built for children, it also has to be quite forgiving and requires save points.</p> <p>The first idea was, of course, to use fonts or SVG. But this solution makes it difficult to balance the game and the implementation is tough. Therefore, I opted for a heatmap-based solution.</p> <p>The idea is that each new line has its own tracing data with start position, end position, checkpoints, order, etc. Once this data is filled in, the directional and distance heatmaps are generated using a small Python tool and provide a JSON that can be used as a lookup table at runtime. This solution isn't perfect but drastically lowers computation time and has the advantage of being very flexible, which is important to balance the game for children.</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#progression-menu","title":"Progression Menu","text":"<p>The progression menu seems trivial, but the Godot Beta version we used did not support scrolling a large texture, especially with the low compute power of the targeted tablet. Therefore, implementing this menu was a challenge both for the spline and background textures. It required developing asynchronous loading/unloading of textures as well as a custom spline component.</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/mz_kalulu/#data-validation","title":"Data Validation","text":"<p>As stated in the project description, game data came from Inserm in the form of very large GSheets and separate files (especially audio files). To simplify the game and the tests, it was decided to check data prior to injecting it into the game. I developed a suite of complex checks in Python to ensure data specifications were respected and could be used safely in the game. It was a difficult topic due to the nature of the data, which had many circular and nested dependencies.</p>","tags":["Godot","GDScript","Python","Git"]},{"location":"posts/professional_projects/sw_expert_road_safety/","title":"Expert Road Safety","text":"","tags":["Unreal Engine 4","Blueprint","C++","VR","Git"]},{"location":"posts/professional_projects/sw_expert_road_safety/#description","title":"Description","text":"<p>Expert Road Safety is a serious game developed by SoWhen?. The main goal is to raise awareness about road safety through a short immersive game in virtual reality. The objective is to put players in a situation where they have to find the causes of a car accident.</p> <p>There are two investigators in VR using HTC Vive and one analyst who plays on a PC. Investigators can explore the accident scene to find clues, while the analyst can listen to testimonies, perform medical analysis, and review clues found by the investigators. The analyst can then guide the investigators to new clues using the map.</p> <p>At the end of the timer, players can gather together to deduce the causes of the accident.</p> <p>Both the VR and analyst experiences were built using Unreal Engine 4.</p> <p></p> <p></p> <p> </p>","tags":["Unreal Engine 4","Blueprint","C++","VR","Git"]},{"location":"posts/professional_projects/sw_expert_road_safety/#personal-contribution","title":"Personal Contribution","text":"<p>The VR application was quite simple, with only a few interactive objects. However, it was still a challenge in terms of graphics performance and required careful development to handle network messaging between the VR and analyst applications.</p> <p>The analyst application was more challenging. The application is almost entirely made of user interface, but we wanted to create a reusable interface that could support multiple road safety maps and experiences. Therefore, the UI was fed using data tables and automatically adapted to the number of elements in each panel. The second difficult aspect was leveraging the use of a specific device as it was running on a touchscreen PC that was also equipped with a small retroprojector facing the desktop. It was part of the experience to leverage multiple display and input methods.</p>","tags":["Unreal Engine 4","Blueprint","C++","VR","Git"]},{"location":"posts/professional_projects/sw_smart_mirror/","title":"Smart Mirror","text":"","tags":["Unreal Engine 4","Unity","OpenCV","Blueprint","C++","C#","Git"]},{"location":"posts/professional_projects/sw_smart_mirror/#description","title":"Description","text":"<p>I worked on an R&amp;D smart mirror project for SoWhen. The device consisted of a bright screen, a one-way mirror, and a depth camera. The idea was to display both 2D and 3D content with a correct perspective for the user. To achieve this, I used the depth camera to track the user's eyes. Additionally, the mirror was designed as an agnostic device capable of using different models of depth cameras and various rendering engines for the final display.</p> <p>Technically, the application took the form of a C++17 application with core interfaces for cameras and rendering engines. I also developed integrations for Kinect and Intel RealSense cameras, as well as plugins for both Unity and Unreal Engine 4.</p> <p>Eye tracking, as well as a prototype for hand tracking, was extracted from the camera using OpenCV. The user's cyclopean eye position was transferred to the rendering engine to compute the projection matrix. The most complex use case was transforming the entire screen into a \"window\" to a virtual world, which required an asymmetric frustum. To transfer images and data in real time across applications, I leveraged computer shared memory.</p>","tags":["Unreal Engine 4","Unity","OpenCV","Blueprint","C++","C#","Git"]},{"location":"posts/tutorials/material_hlsl/","title":"UE HLSL in Materials","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#custom-node","title":"Custom Node","text":"<p>The easiest method to add HLSL in Unreal Engine Materials is to write our code directly in the Custom Node.</p> <p></p> <p>Despite being simple it has several noticeable features. - HLSL can be written directly in the node. - Return type can be selected from float1 up to float4 and can also be a material attributes. - It accept any number of named inputs. Input types are unspecified so its important to match HLSL with provided parameters. - It provides a list of named and typed additional outputs. They can be assigned directly in HLSL. - You can provide additional define with name and value. For example a define named <code>MAGENTA</code> with a value of <code>float3(1, 0, 1)</code> is equivalent to HLSL <code>#define MAGENTA float3(1, 0, 1)</code>. - It can import shader files. This is mostly used to import Unreal Shader Headers. </p> <p>While custom node allow for a quick start with hlsl, it quickly becomes constraining as the code becomes more complex. Writing the code in the material is impractical, does not truly support struct and functions (despite some workaround exists), and does not allow proper version control. Fortunately there is a simple solution, adding our own hlsl shader files into Unreal Engine.  </p> <p>These file comes in two difference format, ush (Unreal Shader Headers) are used to create libraries, while usf (Unreal Shader Format) contains shader entry points. </p> <p>Here we are only looking at ush files which are safe to import from material custom nodes. </p> <p>If you want to add your own shader to Unreal Engine you will need usf files but this is a more complex topic that would requires its own post.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#adding-shader-files","title":"Adding Shader Files","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#from-project","title":"From Project","text":"<p>In order to include our own shader files we need a C++ module. Even if you are not familiar with C++ we only need few lines of code so you should be able to follow up.</p> <p>If your project does not have C++ yet, in main window go to <code>Tools -&gt; New C++ Class</code>. You can chose whatever class you need or just use <code>Empty</code>. Generate and open the solution with your favorite IDE. </p> <p>Open <code>{ProjectName}.h</code> file. It should contain a module declaration similar to the following one. (Replace '' with your project name).</p> <pre><code>#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"Modules/ModuleManager.h\"\n\nclass FProjectNameModule : public IModuleInterface\n{\npublic:\n\n    virtual void StartupModule() override;\n    virtual void ShutdownModule() override;\n};\n</code></pre> <p>Open <code>{ProjectName}.cpp</code> file. It should contain StartupModule and ShutdownModule definitions as well as module declaration. We are only interested in StartupModule method in which we will declare our shader file virtual path.</p> <pre><code>void FRayMarchModule::StartupModule()\n{\n    FString ShaderDir =  FPaths::Combine(FPaths::ProjectDir(), \"FolderName\");\n    if(FPaths::DirectoryExists(ShaderDir))\n    {\n        AddShaderSourceDirectoryMapping(TEXT(\"/VirtualPath\"), ShaderDir);\n    }\n}\n</code></pre> <p>Your shader files should be in <code>ProjectDir/FolderName/</code>. You can change the location if needed. </p> <p>The <code>FolderName</code> is where you plan to store your shader files and I recommend sticking to <code>Shaders</code> as it is the Unreal Engine nomenclature.   </p> <p>The first parameter of <code>AddShaderSourceDirectoryMapping</code> is your virtual shader path name and is important for following steps so you should choose something meaningful for you.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#from-a-plugin","title":"From a Plugin","text":"<p>As an alternative you can also add shader file from a plugin. In this case you can build the path using the plugin manager.</p> <pre><code>void FRayMarchModule::StartupModule()\n{\n    FString PluginShaderDir = FPaths::Combine(\n        IPluginManager::Get().FindPlugin(TEXT(\"PluginName\"))-&gt;GetBaseDir(), TEXT(\"FolderName\")\n    );\n    AddShaderSourceDirectoryMapping(TEXT(\"/VirtualPath\"), PluginShaderDir);\n}\n</code></pre> <p>Your shader files should be in <code>PluginDir/FolderName/</code>. You can change the location if needed. </p> <p>Important</p> <p>Plugin loading phase should be set to <code>PostConfigInit</code>. See <code>.uplugin</code> file.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#using-shader-files","title":"Using Shader Files","text":"<p>Now that our virtual path is declared we can start using hlsl code from shader files.</p> <p>In order to test that let's create an unreal shader header file into  <code>FolderName/Test.ush</code> and create a test function.</p> <pre><code>#pragma once\n#include \"/Engine/Public/Platform.ush\"\n\nfloat3 GetTestColor()\n{\n    return float3(1, 0, 1); // Magenta\n}\n</code></pre> <p>Important</p> <p>All shader files has to include <code>/Engine/Public/Platform.ush</code> directly or indirectly. This ensure Unreal Engine compile your shader for all platforms.</p> <p>In a material let's add a Custom node and add <code>/VirtualPath/Test.ush</code> to its <code>Include File Paths</code>.</p> <p></p> <p>Of course you can also import your shader files in one another or import Unreal Engine Shader files. I especially recommend to check the content of <code>/Engine/Private/Common.ush</code> which contains a lot of useful stuff. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#development","title":"Development","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#development-mode","title":"Development Mode","text":"<p>I recommend setting <code>r.ShaderDevelopmentMode</code> console command to <code>1</code> which enable detailed logs on shader compiles and the opportunity to retry on errors. </p> <p>During development i highly recommend adding it by default in <code>Config/ConsoleVariables.ini</code> so it is automatically enabled on project start up.</p> <pre><code>[Startup]\nr.ShaderDevelopmentMode=1\n</code></pre>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#shader-recompile","title":"Shader Recompile","text":"<p>Unreal Engine Editor automatically recompile modified ush file. However you might need to trigger it manually. </p> <p>By default the shortcut is <code>Ctrl + Shift + .</code> but you can change it by searching <code>Recompile Changed Shaders</code> in <code>Editor Preferences</code>.</p> <p>You can also recompile shaders from command line using <code>RECOMPILESHADERS CHANGED</code>.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#material-recompile","title":"Material Recompile","text":"<p>Unfortunately even if ush file recompiled it won't update materials that use it through custom node. According to the documentation, the solution is to make a dummy change in the material and use Save or Apply button of the toolbar to trigger its recompile. </p> <p>Tip</p> <p>I prefer working directly on the scene so I recompile through an Editor Utility Widget using a Button and a Material Single Property View. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#rider","title":"Rider","text":"<p>If you are using Rider you can associate usf/ush using Rider Associate File Type (might need to restart Rider).</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/material_hlsl/#references","title":"References","text":"<ul> <li>Unreal Engine Shader Documentation</li> <li>Unreal Engine Debugging the Shader Compile Process</li> </ul>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/","title":"UE Ray Marching Introduction","text":"<p>Animation by Inigo Quilez via Shadertoy Licensed under MIT License.</p> <p></p> <p>Animation by Frank Hugenroth via Shadertoy Licensed under CC BY-NC-SA 3.0.</p> <p></p> <p>Animation by Eddie Lee via Shadertoy</p> <p> </p> <p>Animation by Dave Hoskins via Shadertoy</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#what-is-ray-marching","title":"What Is Ray Marching","text":"<p>Ray marching is a group of 3D rendering technics where rays are sampled iteratively segment by segment, computing some function at each step. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#use-cases","title":"Use Cases","text":"<ul> <li>Voxel Rendering</li> <li>Volumetric Rendering</li> <li>Fluid Rendering</li> <li>Screen Space Reflection</li> <li>Screen Space Shadows </li> <li>Physics Simulation</li> <li>Morphing Shapes</li> <li>Geometry Repetition</li> <li>Algorithmically defined Scenes</li> </ul>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#signed-distance-functionsfields","title":"Signed Distance Functions/Fields","text":"<p>Ray marching often takes advantage of SDFs in order to improve sampling or as scene representation. </p> <p>A signed distance function returns the shortest distance from any point in space to the surface of a shape. The sign indicates whether the point is inside (negative), on (zero), or outside (positive) the shape. Complex shape can be assemble from SDF primitives using simple math function.</p> <p>A signed distance field is a 3D (or 2D) spatial representation where each point stores the signed distance to the nearest surface. It can be calculated procedurally from signed distance functions or stored in a texture/volume.</p> <p>In this article SDF refers to signed distance function</p> <p>SDF can be quite frightening for people who don't like math but fortunately we can use them without diving into the maths. However, for those who want to understand that part, lets detail some examples to see how SDFs are built.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#examples","title":"Examples","text":"<pre><code>// p : position\n// s : scale\nfloat sdSphere(float3 p, float s)\n{\n    return length(p) - s;\n}\n\n// p : position\n// b : bounds/extent\nfloat sdBox(float3 p, float3 b)\n{\n    float3 q = abs(p) - b;\n    return length(max(q, 0.0)) + min(max(q.x, max(q.y, q.z)), 0.0);\n}\n\n// d1 : distance1 (sdf1)\n// d2 : distance2 (sdf2)\nfloat opUnion(float d1, float d2)\n{\n    return min(d1, d2);\n}\n</code></pre>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#sampling","title":"Sampling","text":"<p>Rays can be sample in multiple ways depending on the application. The most na\u00efve approach is to sample rays at fixed intervals, which heavily impact performance and precision. The second approach is to use distance aware sampling method that adapt step interval based on the scene being rendered.  </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#sphere-tracing","title":"Sphere Tracing","text":"<p>Sphere tracing idea is too take advantage of SDFs to evaluate the distance from current sampling point to nearest surface. The next sampling point can be moved by this distance along the ray without missing any surface. When the evaluated distance is bellow a threshold, then the ray is close enough to a surface and the sampling must stop and return evaluated surface color.</p> <p></p> <p>Image by Teadrinker, via Wikimedia Commons Licensed under CC BY-SA 4.0.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#volumetric","title":"Volumetric","text":"<p>In volumetric ray marching, all sampled point contribute to final color. For example this can be used to render clouds, gaz or liquids. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#references","title":"References","text":"<p>If you want to learn more about ray marching or SDF I highly recommend to check Inigo Quilez work https://iquilezles.org/. On his website you can also found collection of 2D SDFs and 3D SDFs.</p> <p>As a deeper introduction to Ray Marching you can also check : - Ray Marching Introduction From SimonDev  - Ray Marching Introduction From Jamie Wong</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#unreal-engine-quick-start","title":"Unreal Engine Quick Start","text":"<p>The easiest method to start playing with ray marching in Unreal Engine is to use Custom Node in a Material. It can either be a mesh material or a post process material. In order to render a single object mesh material is more flexible and has a lower rendering cost. Switching to post process become interesting if you need to render multiple objects or if you want to render translucent ray marched volume on top of each others.</p> <p>Let's start by ray marching a simple sdf shape with fixed step sampling. </p> <p>We need : - A Material - A Custom Node with 3 parameters :     - Ray Start Location     - Ray Direction     - Ray Speed / Interval - Sphere signed distance function - Ray marching sampling code</p> <p></p> <p>The actual mesh (rendered in black) can be hidden using opacity (or opacity mask)</p> <p></p> <pre><code>for(int i = 0; i &lt; 1024; i++)\n{\n    // Ray Sampling \n    float3 Position = RayStart + RayDir * i * RaySpeed;\n\n    // Sphere SDF\n    float Distance = length(Position) - 45.0f;\n\n    // Sphere Surface is close enough\n    if(Distance &lt; RaySpeed)\n    {\n        return float4(1.0f, 1.0f, 1.0f, 1.0f);\n    }\n}\n\nreturn float4(0.0f, 0.0f, 0.0f, 0.0f);\n</code></pre> <p>The ray marching renderer is quite simple. We start from the Camera Position and move along the Camera Direction incrementing by a fixed distance (Ray Speed) at each step. Once we are \"on\" the shape (<code>Distance &lt; Threshold</code>) we return sphere color and opacity, if ray does not touch the shape after a fix number of iteration then we return black/masked color. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#ray-direction","title":"Ray Direction","text":"<p>In unreal engine the camera direction is defined from world position to camera. Therefore, it has to be inverted to get a vector that point towards the world. It is also worth noticing that the ray direction vector has to be normalize which is already the case for unreal camera direction vector.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#iteration-and-ray-speed","title":"Iteration and Ray Speed","text":"<p>The number of iteration have a huge impact on performances as each pixel run all iteration unless it reach the sdf.</p> <p>If the camera distance to the shape is greater than NbIteration * RaySpeed then no pixel will be render because the algorithm will reach NbIteration before the ray can reach the shape.</p> <p>Increasing Ray Speed increase maximum rendering distance but decrease rendering precision and quality. Ray speed is also linked to shape distance threshold because it make no sense to have a threshold smaller that the sampling distance which would implies that the sampling can jump over the test. In this example I used RaySpeed (1.0f) as a threshold for shape detection which means that detail lower than 1cm (unreal engine unit) won't be rendered. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#fixing-scale","title":"Fixing Scale","text":"<p>The scaling of the sphere seems wrong. I used a radius of 45cm so we would expect the sdf sphere to be almost the same size as the preview mesh but it is not. This is because material preview meshes have a different size than basic shapes used in the scene. It is easier to work on sdf geometry if the containing mesh have a known size. </p> <p></p> <p>After swapping the mesh to default sphere shape.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#fixing-position","title":"Fixing Position","text":"<p>This first example does not take into account the shape location so the rendered sphere is always at world origin independently of the object location therefore it should render full black/masked unless the object you want to render is at world origin. </p> <p>There are two ways of moving SDF location. We can modify the SDF directly but this will be impracticable when we will combine multiple SDFs to create complex shapes. As we want the sample relative position to the SDF, a much better approach is to modify the sampled position so it is relative to SDF location rather than the origin.</p> <pre><code>// we keep the SDF on origin\nfloat SdSphere(float3 Pos, float Radius)\n{\n    return length(Pos) - Radius;\n}\n// sdf is called using relative position\nfloat Distance = SdSphere(RayPosition - ObjectPosition, 45.0f);\n</code></pre> <p>I add Object World Position to the Custom Node and use the relative position to compute the sphere SDF.</p> <p></p> <p>Now the sphere should render properly if move the mesh in the scene.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#sphere-tracing_1","title":"Sphere Tracing","text":"<p>As we saw, using fixed step sampling heavily impact the render distance, precision and the performances. Therefore a much better approach it to use Sphere Tracing. As shown in the first chapter, this technic use an adaptive sampling distance based on the SDF being rendered which have several advantages.</p> <p>The threshold is not tied anymore to the ray fixed step and can be much more precise. </p> <p>The ray move through the map really quickly when far away from the SDF therefore reducing the number of iteration required. </p> <pre><code>float TotalDistance = 0;\nfloat Threshold = 0.01;\n\nfor(int i = 0; i &lt; 32; i++)\n{\n    // Ray Sampling\n    float3 Position = RayStart + RayDir * TotalDistance;\n\n    // Relative Position\n    float3 RelativePosition = Position - ShapePosition;\n\n    // Sphere SDF\n    float Distance = length(RelativePosition) - ShapeScale;\n\n    // Sphere Surface is close enough\n    if(Distance &lt; Threshold)\n    {\n        return float4(1.0f, 1.0f, 1.0f, 1.0f);\n    }\n\n    // Adaptive distance for next step\n    TotalDistance += Distance;\n}\n\nreturn float4(0.0f, 0.0f, 0.0f, 0.0f);\n</code></pre> <p>In this implementation I used a total distance that grows adaptively to replace the fixed steps based on iteration index. </p> <p>It is worth noticing that : - The number of iteration has decrease significantly.  - The threshold is now really precise.  - The camera distance to the shape is not constrained anymore.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_introduction/#resources","title":"Resources","text":"<ul> <li>SDF 2D</li> <li>SDF 3D</li> <li>Ray Marching Intro Video</li> <li>Ray Marching Intro</li> <li>PBR Introduction</li> <li>PBR in detail</li> <li>Scratch a Pixel Volume Rendering Course</li> <li>Shader Bits Volumetric Ray Marching</li> <li> <p>Moana Water Tutorial</p> </li> <li> <p>Scene View Extension Template (Unreal Global Shader)</p> </li> </ul>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/","title":"UE Ray Marching Lava Lamp","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#start-up","title":"Start Up","text":"<p>I assume you have prior knowledge about Ray Marching and Unreal Engine Shader Files. If it is not the case you can look at my previous posts on these topics : - Introduction of Ray Marching in Unreal Engine  - Moving Material Custom Node HLSL to Shader Files.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#creating-ray-marching-library-header","title":"Creating Ray Marching Library Header","text":"<p>Add Unreal Shader Header File <code>RayMarchingLibrary.ush</code> to the project. I will use this file for reusable functions and structures like SDFs, math tools, or rendering utilities.</p> <p>Let's start with sphere SDF. Here I chose to encapsulate the SDF into a struct for future reuse but it is not required. We should also include <code>Platform.ush</code> as it is mandatory. I also include <code>Common.ush</code> as it will be helpful later on.</p> <pre><code>#pragma once\n#include \"/Engine/Public/Platform.ush\"\n#include \"/Engine/Private/Common.ush\"\n\nstruct SDF\n{\n    static float Sphere(float3 pos, float radius)\n    {\n        return length(pos) - radius;\n    }\n};\n</code></pre>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#creating-lava-lamp-header","title":"Creating Lava Lamp Header","text":"<p>Add Unreal Shader Format File <code>LavaLamp.ush</code> to the project. I will use it for the main rendering loop of the lava lamp. I have decided to split up the lava lamp in a separate file so the library stays clean and reusable.  </p> <p>Depending on your need, each parameter can be declared from the code or exposed it as a parameter in your material. Some parameters like the camera position, camera vector or time are simple to get from the material but might be more difficult to get from HLSL. These values usually comes from <code>Common.ush</code> or the <code>MaterialParameters</code> structure. I personally chose to expose all values that are not trivial to retrieve from HLSL for simplicity. </p> <pre><code>#pragma once\n#include \"./RayMarchingLibrary.ush\"\n\n#define DIST_THRESHOLD 0.1f\n#define ITER_MAX 128\n\nfloat GetScene(float3 Pos, float Time)\n{\n    return FSDF::Sphere(Pos, 25.0f);\n}\n\nfloat4 RenderLavaLamp(float3 RayStart, float3 RayDir, float3 ObjectPos, float Time)\n{\n    float TotalDistance = 0;\n\n    for(int i = 0; i &lt; ITER_MAX; i++)\n    {\n        // Ray Sampling\n        float3 RayPos = RayStart + RayDir * TotalDistance;\n\n        // Ray relative to lava lamp\n        float3 Pos = RayPos - ObjectPos;\n\n        // SDF\n        float Dist = GetScene(Pos, Time);\n\n        // Surface is close enough\n        if(Dist &lt; DIST_THRESHOLD)\n        {\n            return float4(1.0f, 1.0f, 1.0f, 1.0f);\n        }\n\n        // Adaptive distance for next step\n        TotalDistance += Dist;\n    }\n\n    return float4(0.0f, 0.0f, 0.0f, 0.0f);\n}\n</code></pre> <p>Here we don't need to include <code>Platform.ush</code> because it is indirectly included through the library.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#creating-lava-lamp-material","title":"Creating Lava Lamp Material","text":"<p>Create an unlit translucent material <code>M_LavaLamp</code> and set the preview window to unlit for consistency. Add a Custom Node that includes <code>LavaLamp.ush</code> using your virtual path and returns <code>RenderLavaLamp()</code>. </p> <p></p> <p>As we took Object World Position into account you should have the same result anywhere in a map. </p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#prerequisites","title":"Prerequisites","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#first-rendering","title":"First Rendering","text":"<p>In order to work on geometry we need a simple renderer that provides some sens of depth. The first step of any rendering is to get scene normals using difference approximation. </p> <pre><code>// Central Difference Approximation : (P+ - P-) / 2\nfloat3 GetNormal(float3 Pos) \n{\n    return normalize(0.5 * float3(\n        GetScene(Pos + float3(0.1f, 0.0f, 0.0f)) - GetScene(Pos - float3(0.1f, 0.0f, 0.0f)),\n        GetScene(Pos + float3(0.0f, 0.1f, 0.0f)) - GetScene(Pos - float3(0.0f, 0.1f, 0.0f)),\n        GetScene(Pos + float3(0.0f, 0.0f, 0.1f)) - GetScene(Pos - float3(0.0f, 0.0f, 0.1f))\n    ));\n}\n\n// Forward Difference Approximation : (P+ - P)\nfloat3 GetCheapNormal(float3 Pos, float SceneDist) \n{\n    return normalize(float3(\n        GetScene(Pos + float3(0.1f, 0.0f, 0.0f)) - SceneDist,\n        GetScene(Pos + float3(0.0f, 0.1f, 0.0f)) - SceneDist,\n        GetScene(Pos + float3(0.0f, 0.0f, 0.1f)) - SceneDist\n    ));\n}\n</code></pre> <p>The delta (here '0.1') needs to match your object geometry. Small object and high curvature requires small numbers. Most of the time forward difference approximation has enough precision and should be preferred for performance reasons.</p> <p>Using the normal as color is really effective to work on geometry but we can also make a simple fresnel effect from it.</p> <pre><code>if(Dist &lt; DIST_THRESHOLD)\n{\n    // Use normal directly\n    return float4(Normal, 1.0f);\n\n    // Fresnel effect\n    float Fresnel = pow(dot(RayDir, Normal), 2);\n    return float4(Fresnel, Fresnel, Fresnel, 1.0f);\n}\n</code></pre> <p>Fresnel effect uses the dot product which is 0 when vectors are perpendicular and 1/-1 when they are aligned. The sign doesn't matter due to the power of 2.</p> <p> </p> <p>Normal</p> <p></p> <p>Fresnel effect</p> <p>In order to see the container (here a cylinder) i just used a small alpha value as default.</p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#generating-random-numbers","title":"Generating Random Numbers","text":"<p>In order to work on our geometry we need deterministic random number generator. In HLSL there is no built-in random function so the common technic is to use hash functions.</p> <p>These function typically take one or multiple numbers as an input and output a pseudo random number from it. This mean that if we feed the same input we get the same output. One typical input is to use UV coordinate so random numbers are deterministic pixel wise. In our case we are going to generate hash input manually. </p> <pre><code>// using float input\nfloat Rand(float x)\n{\n    return frac(sin(x) * 43758.5453123);\n}\n\n// using integer input\nfloat Rand(int x)\n{\n    uint n = asuint(x);\n    n = (n &lt;&lt; 13) ^ n;\n    return frac((n * (n * n * 15731u + 789221u) + 1376312589u) * (1.0 / 4294967296.0));\n}\n\n// Signed Rand from float\nfloat SRand(float X)\n{\n    return Rand(X) * 2.0f - 1.0f;\n}\n\n// Signed Rand from int\nfloat SRand(int X)\n{\n    return Rand(X) * 2.0f - 1.0f;\n}\n</code></pre>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#geometry","title":"Geometry","text":"<p>For this example I have chosen to use cylindrical shape with a blob at the bottom.</p> <p>First step is to add SDF we need in the library. </p> <pre><code>struct FSDF\n{\n    static float Sphere(float3 pos, float radius)\n    {\n        return length(pos) - radius;\n    }\n\n    static float Cylinder(float3 p, float h, float r)\n    {\n        float2 d = abs(float2(length(p.xy), p.z)) - float2(r, h);\n        return min(max(d.x, d.y), 0.0) + length(max(d, 0.0));\n    }\n\n    static float OpUnion(float d1, float d2)\n    {\n        return min(d1, d2);\n    }\n\n    static float OpSmoothUnion(float d1, float d2, float k)\n    {\n        float h = max(k - abs(d1 - d2), 0.0f);\n        return min(d1, d2) - h * h * 0.25 / k;\n    }\n\n    static float OpSmoothIntersection(float d1, float d2, float k)\n    {\n        float h = clamp(0.5 - 0.5 * (d2 - d1) / k, 0.0, 1.0);\n        return lerp(d2, d1, h) + k * h * (1.0 - h);\n    }\n};\n</code></pre> <p>Then we need to get time in order to make our geometry moves. Simplest method is to add it to <code>RenderLavaLamp</code> function and bind it to <code>Time</code> in the material. We also need to add time as parameter to all function that needs it (GetScene, GetNormal...).</p> <pre><code>float4 RenderLavaLamp(float3 RayStart, float3 RayDir, float3 ObjectPos, float Time)\n</code></pre> <p>Now we can create our geometry in the GetScene function. Here is my version as an example. </p> <pre><code>#define GLASS_HALF_HEIGHT 20\n#define GLASS_RADIUS 10\n#define BUBBLE_SPEED 0.25f\n\nfloat GetScene(float3 Pos, float Time)\n{\n    // Deterministic Random Index\n    int RandId = 0;\n\n    // Cylinder Glass Container\n    float SdGlass = FSDF::Cylinder(Pos, GLASS_HALF_HEIGHT, GLASS_RADIUS);\n\n    // Blob at the bottom of the container \n    float SdBottomBubble = FSDF::Sphere(Pos - float3(0, 0, -(GLASS_HALF_HEIGHT + GLASS_RADIUS * 1.4)), GLASS_RADIUS * 2);\n\n    // All bubbles\n    float SdBubbles = SdBottomBubble;\n    for(int i = 0; i &lt; 10; i++)\n    {\n        float3 BubbleLocation = float3(\n            sin(BUBBLE_SPEED * SRand(RandId++) * Time + PI * SRand(RandId++)) * GLASS_RADIUS * 0.80f,\n            sin(BUBBLE_SPEED * SRand(RandId++) * Time + PI * SRand(RandId++)) * GLASS_RADIUS * 0.80f,\n            sin(BUBBLE_SPEED * SRand(RandId++) * Time + PI * SRand(RandId++)) * GLASS_HALF_HEIGHT * 0.90f\n        );\n\n        float BubbleRadius = lerp(3.0f, 7.5f, abs(Rand(RandId++)));\n\n        float SdBubble = FSDF::Sphere(Pos - BubbleLocation, BubbleRadius);\n\n        SdBubbles = FSDF::OpSmoothUnion(SdBubbles, SdBubble, 3);\n    }\n\n    // Constrain bubbles in the glass container\n    return FSDF::OpSmoothIntersection(SdBubbles, SdGlass, 3);\n}\n</code></pre> <p></p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#rendering","title":"Rendering","text":"","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#gradients","title":"Gradients","text":"<p>They are many option for rendering but for this example I have chosen a simple approach.  The idea is to simulate a light coming from the bottom of the lava lamp.</p> <p>Radial Gradient</p> <p>I kept the fresnel effect that gives a nice surface feeling.</p> <pre><code>float RadialGradient = pow(dot(RayDir, Normal), 2);\n</code></pre> <p></p> <p>Height Gradient</p> <pre><code>// return a gradient from 0 at the top of the cylinder to 1 at the bottom\nfloat HeightRatio = 1 - 0.5f * (Pos.z + GLASS_HALF_HEIGHT) / GLASS_HALF_HEIGHT;\n</code></pre> <p></p> <p>Light Gradient</p> <pre><code>// Light source is at the bottom so we check if the normal is pointing down.\n// Remap dot product range [-1, 1] to [0, 1]\nfloat LightGradient = 0.5f * (1 + dot(Normal, float3(0, 0, -1)));\n</code></pre> <p></p> <p>Output</p> <p>There are several ways to combine the gradients but for this example I kept it simple.</p> <p>I used the following method to remap the gradient into acceptable range, in my case <code>[0.15f, 1.0f]</code>.</p> <pre><code>Gradient = lerp(0.15, 1, clamp(Gradient, 0, 1));\n</code></pre> <p>Then combined all gradients with the base color.</p> <pre><code>float3 BaseColor = float3(1, 1, 1);\nfloat3 OutColor = BaseColor * HeightGradient * RadialGradient * LightGradient\n</code></pre> <p></p>","tags":["Unreal Engine 5","HLSL"]},{"location":"posts/tutorials/raymarching_lava_lamp/#adding-colors","title":"Adding Colors","text":"<p>To keep it simple I choose to add colors in the material but doing it in HLSL is viable too. Both options have pros and cons depending on your needs. </p> <p>I modify the custom node output to get the gradients in the material. </p> <pre><code>return float4(RadialGradient, HeightGradient, LightGradient, 1.0f);\n</code></pre> <p></p> <p>I used gradient to compute X coordinate into a heatmap texture. This gives a nice color gradient from red to yellow. I also used height and light gradient to darken \"shadow\" region.  </p> <p></p> <p></p>","tags":["Unreal Engine 5","HLSL"]}]}